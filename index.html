<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zehai He</title>

  <meta name="author" content="Zehai He">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Zehai He
                  </p>
                  <p>
                    I am a senior undergraduate student in Computer Science at Tsinghua University, supervised by Prof. <a href="http://keg.cs.tsinghua.edu.cn/jietang/">Jie Tang</a>. 
                  </p>
                  <p>
                    My research interests focus on multimodal large language models, agents, and reinforcement learning for post-training.
                  </p>

                  <p style="text-align:center">
                    <a href="mailto:he-zh22@mails.tsinghua.edu.cn">Email</a> &nbsp;/&nbsp;
                    <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?user=2-r0RNUAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                    <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                    <!-- <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp; -->
                    <a href="https://github.com/he-zh22">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/ZehaiHe.jpg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/ZehaiHe.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <section class="honors-section">
            <h2>Honors & Awards</h2>
            <ul class="awards-list">
              <li><span class="year">2025:</span> <strong>ICCV 2025 Highlight Paper</strong> for LVBench</li>
              <li><span class="year">2023 & 2024 & 2025:</span> Comprehensive Excellence Scholarship</li>
            </ul>
          </section>

          <br>

          <h2>Selected Research</h2>
          
          <h3>Visual Language Foundation Models</h3>

          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="image-container">
                    <img src="images/GLM_4_5V_and_GLM_4_1V_Thinking_thumbnail.jpg" alt="Framework of GLM-4.5V and GLM-4.1V-Thinking" width="250"
                    style="border-style: none">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2507.01006">
                    <span class="papertitle">GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning
                    </span>
                  </a>
                  <br>
                  <div class="author-list">
                      <span id="visible-authors">
                          Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng,<strong> Zehai He</strong>, Zhe Su, Zhen Yang, Ziyang Pan, ..., Minlie Huang, Yuxiao Dong, Jie Tang
                      </span>
                      <span id="hidden-authors" class="hidden-authors">
                          , Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, Jie Tang
                      </span>
                      <span id="author-count" class="author-count"> (87 authors) </span>
                      <button id="toggle-btn" class="toggle-button">Show all authors</button>
                  </div>
                  <p></p>
                  <a href="https://arxiv.org/abs/2507.01006">ArXiv</a>
                  /
                  <a
                    href="https://github.com/zai-org/GLM-V">GitHub</a>
                  /
                  Models <a href="https://huggingface.co/zai-org/GLM-4.1V-9B-Thinking">4.1V</a>
                   & <a href="https://huggingface.co/zai-org/GLM-4.5V">4.5V</a>
                  /
                  API <a href="https://www.bigmodel.cn/dev/api/visual-reasoning-model/GLM-4.1V-Thinking">4.1V (free)</a>  & <a href="https://www.bigmodel.cn/dev/api/visual-reasoning-model/GLM-4.5V">4.5V</a>
                  /
                  <a href="data/glm_4_1v_thinking.bib">bibtex</a>
                  <p></p>
                  <p>We present <strong> GLM-4.5V </strong> (106B-A12B) and <strong> GLM-4.1V-Thinking </strong> (9B), a series of open-sourced VLMs designed to advance general-purpose multimodal understanding and reasoning. With enhanced pre-trained base and carefully optimized multi-domain RL procedure, GLM-4.5V achieves state-of-the-art performance on nearly all tasks among open-source models of similar size in a comprehensive evaluation across 42 public benchmarks. </p>
                </td>
                  <script>
                    const toggleBtn = document.getElementById('toggle-btn');
                    const hiddenAuthors = document.getElementById('hidden-authors');
                    const authorCount = document.getElementById('author-count');
                    let isExpanded = false;

                    toggleBtn.addEventListener('click', function() {
                        if (isExpanded) {
                            // Collapse
                            hiddenAuthors.style.display = 'none';
                            authorCount.style.display = 'inline';
                            toggleBtn.textContent = 'Show all authors';
                            isExpanded = false;
                        } else {
                            // Expand
                            hiddenAuthors.style.display = 'inline';
                            authorCount.style.display = 'none';
                            toggleBtn.textContent = 'Show fewer authors';
                            isExpanded = true;
                        }
                    });
                </script>
              </tr>

            </tbody>
          </table>

          <h3>Evaluation of Vision Language Models</h3>

          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="image-container">
                    <img src="images/lvbench_thumbnail.jpg" alt="LVBench intro" width="250"
                    style="border-style: none">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2406.08035">
                    <span class="papertitle">LVBench: An Extreme Long Video Understanding Benchmark
                    </span>
                  </a>
                  <br>
                  Weihan Wang, <strong> Zehai He </strong>, Wenyi Hong , Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang

                  <br>
                  <em>ICCV</em>, 2025 
                  <br>
                  <a href="https://arxiv.org/pdf/2406.08035">ArXiv</a>
                  /
                  <a href="https://lvbench.github.io/"> Project Page </a>
                  /
                  <a href="https://huggingface.co/datasets/THUDM/LVBench">Dataset</a>
                  /
                  <a href="data/lvbench.bib">bibtex</a>
                  <p></p>
                  <p>
                    We introduce LVBench, a benchmark specifically designed for long video understanding. Our dataset contains 6 major capability categories and 21 subcategories, with the video average length of 1.14 hours,  approximately four times longer than the longest existing dataset.
                  </p>
                </td>
              </tr>


            </tbody>
          </table>

          

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    Reference code: <a href="https://github.com/jonbarron/jonbarron_website">source
                      code</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>